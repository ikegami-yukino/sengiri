from nose.tools import assert_equal
import sengiri

def test_tokenize():
    actual = sengiri.tokenize('ã“ã‚Œã¯ï¼(ã™ã°ã‚‰ã—ã„ï¼)æ„Ÿå‹•â€¦â€¦ã€‚')
    assert_equal(actual, ['ã“ã‚Œã¯ï¼', '(ã™ã°ã‚‰ã—ã„ï¼)', 'æ„Ÿå‹•â€¦â€¦ã€‚'])
    actual = sengiri.tokenize('ã†ãƒ¼ã‚“ğŸ¤”ğŸ¤”ğŸ¤”ã©ã†ã—ã‚ˆã†')
    assert_equal(actual, ['ã†ãƒ¼ã‚“ğŸ¤”ğŸ¤”ğŸ¤”', 'ã©ã†ã—ã‚ˆã†'])
    actual = sengiri.tokenize('ãƒ¢ãƒ¼å¨˜ã€‚ã®ã‚³ãƒ³ã‚µãƒ¼ãƒˆã«è¡Œã£ãŸã€‚')
    assert_equal(actual, ['ãƒ¢ãƒ¼å¨˜ã€‚ã®ã‚³ãƒ³ã‚µãƒ¼ãƒˆã«è¡Œã£ãŸã€‚'])
    actual = sengiri.tokenize('æ¥½ã—ã‹ã£ãŸã—å¬‰ã—ã‹ã£ãŸã€‚ã™ã”ãå……å®Ÿã—ãŸ!')
    assert_equal(actual, ['æ¥½ã—ã‹ã£ãŸã—å¬‰ã—ã‹ã£ãŸã€‚', 'ã™ã”ãå……å®Ÿã—ãŸ!'])
